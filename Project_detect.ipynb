{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def read_data(file):\n",
    "    data = []\n",
    "    with open(file, 'r')as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            label = ' '.join(line[1:line.find(\"]\")].strip().split())\n",
    "            text = line[line.find(\"]\")+1:].strip()\n",
    "            data.append([label, text])\n",
    "    return data\n",
    "\n",
    "file = 'text.txt'\n",
    "data = read_data(\"Dataset.txt\")\n",
    "# print(data)\n",
    "# print(\"Number of instances: {}\".format(len(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Function***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‡∏™‡∏£‡πâ‡∏≤‡∏á biggram ‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤\n",
    "def ngram(token, n): \n",
    "    output = []\n",
    "    for i in range(n-1, len(token)): \n",
    "        ngram = ' '.join(token[i-n+1:i+1])\n",
    "        output.append(ngram) \n",
    "    return output\n",
    "\n",
    "# feature ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°\n",
    "def create_feature(text, nrange=(1, 1)):\n",
    "    text_features = [] \n",
    "    text = text.lower() # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏°‡∏û‡πå‡πÄ‡∏•‡πá‡∏Å\n",
    "    text_alphanum = re.sub('[^a-z0-9#]', ' ', text) # ‡πÄ‡∏≠‡∏≤‡∏ï‡∏±‡∏ß‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡∏≠‡∏≠‡∏Å\n",
    "    for n in range(nrange[0], nrange[1]+1): \n",
    "        text_features += ngram(text_alphanum.split(), n)    \n",
    "    text_punc = re.sub('[a-z0-9]', ' ', text)\n",
    "    text_features += ngram(text_punc.split(), 1)\n",
    "    return Counter(text_features) # ‡∏ô‡∏±‡∏ö feature \n",
    "\n",
    "def convert_label(item, name): \n",
    "    items = list(map(float, item.split())) #str to float\n",
    "    label = \"\"\n",
    "    for idx in range(len(items)): \n",
    "        if items[idx] == 1: \n",
    "            label += name[idx] + \" \"\n",
    "    \n",
    "    return label.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'love': 2, 'i': 1, 'programming': 1, 'and': 1, 'dota2': 1, 'i love': 1, 'love programming': 1, 'programming and': 1, 'and love': 1, 'love dota2': 1, 'i love programming': 1, 'love programming and': 1, 'programming and love': 1, 'and love dota2': 1, '.': 1})\n"
     ]
    }
   ],
   "source": [
    "# test function\n",
    "text = \"I love programming and love dota2.\"\n",
    "features = create_feature(text, nrange=(1, 3))\n",
    "\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Encode text***\n",
    "use to train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "\n",
    "X_all = []\n",
    "y_all = []\n",
    "for label, text in data:\n",
    "    y_all.append(convert_label(label, emotions))\n",
    "    X_all.append(create_feature(text, nrange=(1, 4)))\n",
    "\n",
    "# print(X_all[0])\n",
    "# print(X_all[1])\n",
    "# # print(\"-------\")\n",
    "# print(y_all[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.2, random_state = 123)\n",
    "\n",
    "# ‡∏Ñ‡∏¥‡∏î accuracy\n",
    "def train_test(clf, X_train, X_test, y_train, y_test):\n",
    "    clf.fit(X_train, y_train) #train\n",
    "    train_acc = accuracy_score(y_train, clf.predict(X_train))\n",
    "    test_acc = accuracy_score(y_test, clf.predict(X_test))\n",
    "    return train_acc, test_acc\n",
    "\n",
    "vectorizer = DictVectorizer(sparse = True) #dictionary \n",
    "# to vector\n",
    "X_train = vectorizer.fit_transform(X_train) \n",
    "X_test = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Classifier                | Training Accuracy | Test Accuracy |\n",
      "| ------------------------- | ----------------- | ------------- |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:1242: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| LinearSVC                 |         0.9988302 |     0.5768717 |\n",
      "| RandomForestClassifier    |         0.9988302 |     0.5541444 |\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(random_state=123)\n",
    "rforest = RandomForestClassifier(random_state=123)\n",
    "\n",
    "clifs = [lsvc, rforest]\n",
    "\n",
    "# train and test them \n",
    "print(\"| {:25} | {} | {} |\".format(\"Classifier\", \"Training Accuracy\", \"Test Accuracy\"))\n",
    "print(\"| {} | {} | {} |\".format(\"-\"*25, \"-\"*17, \"-\"*13))\n",
    "for clf in clifs: \n",
    "    clf_name = clf.__class__.__name__\n",
    "    train_acc, test_acc = train_test(clf, X_train, X_test, y_train, y_test)\n",
    "    print(\"| {:25} | {:17.7f} | {:13.7f} |\".format(clf_name, train_acc, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "joy       (1. 0. 0. 0. 0. 0. 0.)  1084\n",
      "anger     (0. 0. 1. 0. 0. 0. 0.)  1080\n",
      "sadness   (0. 0. 0. 1. 0. 0. 0.)  1079\n",
      "fear      (0. 1. 0. 0. 0. 0. 0.)  1078\n",
      "disgust   (0. 0. 0. 0. 1. 0. 0.)  1057\n",
      "guilt     (0. 0. 0. 0. 0. 0. 1.)  1057\n",
      "shame     (0. 0. 0. 0. 0. 1. 0.)  1045\n"
     ]
    }
   ],
   "source": [
    "# count data\n",
    "l = [\"joy\", 'fear', \"anger\", \"sadness\", \"disgust\", \"shame\", \"guilt\"]\n",
    "l.sort()\n",
    "label_freq = {}\n",
    "for label, _ in data: \n",
    "    label_freq[label] = label_freq.get(label, 0) + 1\n",
    "\n",
    "# print the labels and their counts in sorted order \n",
    "for l in sorted(label_freq, key=label_freq.get, reverse=True):\n",
    "    print(\"{:10}({})  {}\".format(convert_label(l, emotions), l, label_freq[l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This looks so impressive üò¢\n",
      "I have a fear of dogs üò±\n",
      "My dog died yesterday üò¢\n",
      "I don't love you anymore..! üòÇ\n",
      "I am so tried üò¢\n",
      "I should not do it to him üòû\n"
     ]
    }
   ],
   "source": [
    "emoji_dict = {\"joy\":\"üòÇ\", \"fear\":\"üò±\", \"anger\":\"üò†\", \"sadness\":\"üò¢\", \"disgust\":\"üòí\", \"shame\":\"üò≥\", \"guilt\":\"üòû\"}\n",
    "t1 = \"This looks so impressive\"\n",
    "t2 = \"I have a fear of dogs\"\n",
    "t3 = \"My dog died yesterday\"\n",
    "t4 = \"I don't love you anymore..!\"\n",
    "t5 = \"I am so tried\"\n",
    "t6 = \"I should not do it to him\"\n",
    "\n",
    "texts = [t1, t2, t3, t4 ,t5 ,t6]\n",
    "for text in texts: \n",
    "    features = create_feature(text, nrange=(1, 4))\n",
    "    features = vectorizer.transform(features)\n",
    "    prediction = clf.predict(features)[0]\n",
    "    print( text,emoji_dict[prediction])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
